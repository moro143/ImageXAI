% This file was created with JabRef 2.7b.
% Encoding: UTF-8

@inproceedings{9093360,
    author = {Desai, Saurabh and Ramaswamy, Harish G.},
    booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision
                 (WACV)},
    title = {Ablation-CAM: Visual Explanations for Deep Convolutional Network
             via Gradient-free Localization},
    year = {2020},
    volume = {},
    number = {},
    pages = {972-980},
    keywords = {Visualization;Neurons;Task analysis;Computer architecture;Data
                models;Data visualization;Backpropagation},
    doi = {10.1109/WACV45572.2020.9093360},
}

@misc{ribeiro2016why,
    title = {"Why Should I Trust You?": Explaining the Predictions of Any
             Classifier},
    author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
    year = {2016},
    eprint = {1602.04938},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@misc{lundberg2017unified,
    title = {A Unified Approach to Interpreting Model Predictions},
    author = {Scott Lundberg and Su-In Lee},
    year = {2017},
    eprint = {1705.07874},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI},
}
@article{Selvaraju_2019,
    title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based
             Localization},
    volume = {128},
    ISSN = {1573-1405},
    url = {http://dx.doi.org/10.1007/s11263-019-01228-7},
    DOI = {10.1007/s11263-019-01228-7},
    number = {2},
    journal = {International Journal of Computer Vision},
    publisher = {Springer Science and Business Media LLC},
    author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek
              and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
    year = {2019},
    month = oct,
    pages = {336–359},
}


 @inproceedings{He_2016_CVPR,
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    title = {Deep Residual Learning for Image Recognition},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and
                 Pattern Recognition (CVPR)},
    month = {June},
    year = {2016},
}

    @inproceedings{8379889,
    author = {Jmour, Nadia and Zayen, Sehla and Abdelkrim, Afef},
    booktitle = {2018 International Conference on Advanced Systems and Electric
                 Technologies (IC\_ASET)},
    title = {Convolutional neural networks for image classification},
    year = {2018},
    volume = {},
    number = {},
    pages = {397--402},
    keywords = {Task analysis;Feature extraction;Training;Convolutional neural
                networks;Neurons;Machine learning;Convolution;Convolutional
                neural network;Deep Learning;Transfer Learning;ImageNet},
    doi = {10.1109/ASET.2018.8379889},
}


@article{doi:10.1080/01431160600746456,
    author = {D. Lu and Q. Weng},
    title = {A survey of image classification methods and techniques for
             improving classification performance},
    journal = {International Journal of Remote Sensing},
    volume = {28},
    number = {5},
    pages = {823--870},
    year = {2007},
    publisher = {Taylor \& Francis},
    doi = {10.1080/01431160600746456},
    URL = { https://doi.org/10.1080/01431160600746456 },
    eprint = { https://doi.org/10.1080/01431160600746456 },
}

@inproceedings{medical,
    author = {Li, Qing and Cai, Weidong and Wang, Xiaogang and Zhou, Yun and
              Feng, David Dagan and Chen, Mei},
    booktitle = {2014 13th International Conference on Control Automation
                 Robotics \& Vision (ICARCV)},
    title = {Medical image classification with convolutional neural network},
    year = {2014},
    volume = {},
    number = {},
    pages = {844-848},
    keywords = {Feature extraction;Lungs;Training;Neurons;Biological neural
                networks;Biomedical imaging;Kernel},
    doi = {10.1109/ICARCV.2014.7064414},
}

@article{biology,
    doi = {10.1371/journal.pbio.2005970},
    author = {McQuin, Claire AND Goodman, Allen AND Chernyshev, Vasiliy AND
              Kamentsky, Lee AND Cimini, Beth A. AND Karhohs, Kyle W. AND Doan,
              Minh AND Ding, Liya AND Rafelski, Susanne M. AND Thirstrup, Derek
              AND Wiegraebe, Winfried AND Singh, Shantanu AND Becker, Tim AND
              Caicedo, Juan C. AND Carpenter, Anne E.},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {CellProfiler 3.0: Next-generation image processing for biology},
    year = {2018},
    month = {07},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pbio.2005970},
    pages = {1-17},
    abstract = {CellProfiler has enabled the scientific research community to
                create flexible, modular image analysis pipelines since its
                release in 2005. Here, we describe CellProfiler 3.0, a new
                version of the software supporting both whole-volume and
                plane-wise analysis of three-dimensional (3D) image stacks,
                increasingly common in biomedical research. CellProfiler’s
                infrastructure is greatly improved, and we provide a protocol for
                cloud-based, large-scale image processing. New plugins enable
                running pretrained deep learning models on images. Designed by
                and for biologists, CellProfiler equips researchers with powerful
                computational tools via a well-documented user interface,
                empowering biologists in all fields to create quantitative,
                reproducible image analysis workflows.},
    number = {7},
}

@article{marketing,
    title = {The effects of visual congruence on increasing consumers’ brand
             engagement: An empirical investigation of influencer marketing on
             instagram using deep-learning algorithms for automatic image
             classification},
    journal = {Computers in Human Behavior},
    volume = {112},
    pages = {106443},
    year = {2020},
    issn = {0747-5632},
    doi = {https://doi.org/10.1016/j.chb.2020.106443},
    url = {https://www.sciencedirect.com/science/article/pii/S0747563220301965},
    author = {Young Anna Argyris and Zuhui Wang and Yongsuk Kim and Zhaozheng
              Yin},
    keywords = {Social influence, Similarity attraction model, Influencer
                marketing, Instagram, Deep-learning algorithms for image
                classification, Social media analytics},
    abstract = {Influencers are non-celebrity individuals who gain popularity on
                social media by posting visually attractive content (e.g., photos
                and videos) and by interacting with other users (i.e., Followers)
                to create a sense of authenticity and friendship. Brands partner
                with Influencers to garner engagement from their target consumers
                in a new marketing strategy known as “Influencer marketing.”
                Nonetheless, the theoretical underpinnings of such remains
                unknown. We suggest a new conceptual framework of
                “Visual-Congruence induced Social Influence (VCSI),” which
                contextualizes the Similarity-Attraction Model in the Social
                Influence literature. Using VCSI, we delineate how Influencers
                use visual congruence as representations of shared interests in a
                specific area to build strong bonds with Followers. This intimate
                affiliation catalyzes (i.e., mediates) the positive effects of
                visual congruence on Followers’ brand engagement. To test these
                hypotheses, we conducted in vivo observations of Influencer
                marketing on Instagram. We collected >45,000 images and social
                media usage behaviors over 26 months. We then applied
                deep-learning algorithms to automatically classify each image and
                used social media analytics to disclose hidden associations
                between visual elements and brand engagement. Our hypothesis
                testing results provide empirical support for VCSI, advancing
                theories into the rapidly growing fields of multimodal content
                and Influencer marketing.},
}

@inproceedings{security,
    author = {Akçay, Samet and Kundegorski, Mikolaj E. and Devereux, Michael and
              Breckon, Toby P.},
    booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
    title = {Transfer learning using convolutional neural networks for object
             classification within X-ray baggage security imagery},
    year = {2016},
    volume = {},
    number = {},
    pages = {1057-1061},
    keywords = {X-ray imaging;Training;Support vector machines;Security;Feature
                extraction;Weapons;Visualization;Convolutional neural
                networks;transfer learning;image classification;baggage X-ray
                security},
    doi = {10.1109/ICIP.2016.7532519},
}


@inbook{Koonce2021,
    author = "Koonce, Brett",
    title = "ResNet 50",
    bookTitle = "Convolutional Neural Networks with Swift for Tensorflow: Image
                 Recognition and Dataset Categorization",
    year = "2021",
    publisher = "Apress",
    address = "Berkeley, CA",
    pages = "63--72",
    abstract = "ResNet 50 is a crucial network for you to understand. It is the
                basis of much academic research in this field. Many different
                papers will compare their results to a ResNet 50 baseline, and it
                is valuable as a reference point. As well, we can easily download
                the weights for ResNet 50 networks that have been trained on the
                Imagenet dataset and modify the last layers (called
                **retraining** or **transfer learning**) to quickly produce
                models to tackle new problems. For most problems, this is the
                best approach to get started with, rather than trying to invent
                new networks or techniques. Building a custom dataset and scaling
                it up with data augmentation techniques will get you a lot
                further than trying to build a new architecture.",
    isbn = "978-1-4842-6168-2",
    doi = "10.1007/978-1-4842-6168-2_6",
    url = "https://doi.org/10.1007/978-1-4842-6168-2_6",
}

@article{XAItax,
    title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies,
             opportunities and challenges toward responsible AI},
    journal = {Information Fusion},
    volume = {58},
    pages = {82-115},
    year = {2020},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2019.12.012},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
    author = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier
              {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado
              and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and
              Richard Benjamins and Raja Chatila and Francisco Herrera},
    keywords = {Explainable Artificial Intelligence, Machine Learning, Deep
                Learning, Data Fusion, Interpretability, Comprehensibility,
                Transparency, Privacy, Fairness, Accountability, Responsible
                Artificial Intelligence},
    abstract = {In the last few years, Artificial Intelligence (AI) has achieved
                a notable momentum that, if harnessed appropriately, may deliver
                the best of expectations over many application sectors across the
                field. For this to occur shortly in Machine Learning, the entire
                community stands in front of the barrier of explainability, an
                inherent problem of the latest techniques brought by
                sub-symbolism (e.g. ensembles or Deep Neural Networks) that were
                not present in the last hype of AI (namely, expert systems and
                rule based models). Paradigms underlying this problem fall within
                the so-called eXplainable AI (XAI) field, which is widely
                acknowledged as a crucial feature for the practical deployment of
                AI models. The overview presented in this article examines the
                existing literature and contributions already done in the field
                of XAI, including a prospect toward what is yet to be reached.
                For this purpose we summarize previous efforts made to define
                explainability in Machine Learning, establishing a novel
                definition of explainable Machine Learning that covers such prior
                conceptual propositions with a major focus on the audience for
                which the explainability is sought. Departing from this
                definition, we propose and discuss about a taxonomy of recent
                contributions related to the explainability of different Machine
                Learning models, including those aimed at explaining Deep
                Learning methods for which a second dedicated taxonomy is built
                and examined in detail. This critical literature analysis serves
                as the motivating background for a series of challenges faced by
                XAI, such as the interesting crossroads of data fusion and
                explainability. Our prospects lead toward the concept of
                Responsible Artificial Intelligence, namely, a methodology for
                the large-scale implementation of AI methods in real
                organizations with fairness, model explainability and
                accountability at its core. Our ultimate goal is to provide
                newcomers to the field of XAI with a thorough taxonomy that can
                serve as reference material in order to stimulate future research
                advances, but also to encourage experts and professionals from
                other disciplines to embrace the benefits of AI in their activity
                sectors, without any prior bias for its lack of interpretability.
                },
}

 @artice{XAIcurrent,
    AUTHOR = {Antoniadi, Anna Markella and Du, Yuhan and Guendouz, Yasmine and
              Wei, Lan and Mazo, Claudia and Becker, Brett A. and Mooney,
              Catherine},
    TITLE = {Current Challenges and Future Opportunities for XAI in Machine
             Learning-Based Clinical Decision Support Systems: A Systematic
             Review},
    JOURNAL = {Applied Sciences},
    VOLUME = {11},
    YEAR = {2021},
    NUMBER = {11},
    ARTICLE-NUMBER = {5088},
    URL = {https://www.mdpi.com/2076-3417/11/11/5088},
    ISSN = {2076-3417},
    ABSTRACT = {Machine Learning and Artificial Intelligence (AI) more broadly
                have great immediate and future potential for transforming almost
                all aspects of medicine. However, in many applications, even
                outside medicine, a lack of transparency in AI applications has
                become increasingly problematic. This is particularly pronounced
                where users need to interpret the output of AI systems.
                Explainable AI (XAI) provides a rationale that allows users to
                understand why a system has produced a given output. The output
                can then be interpreted within a given context. One area that is
                in great need of XAI is that of Clinical Decision Support Systems
                (CDSSs). These systems support medical practitioners in their
                clinic decision-making and in the absence of explainability may
                lead to issues of under or over-reliance. Providing explanations
                for how recommendations are arrived at will allow practitioners
                to make more nuanced, and in some cases, life-saving decisions.
                The need for XAI in CDSS, and the medical field in general, is
                amplified by the need for ethical and fair decision-making and
                the fact that AI trained with historical data can be a
                reinforcement agent of historical actions and biases that should
                be uncovered. We performed a systematic literature review of work
                to-date in the application of XAI in CDSS. Tabular data
                processing XAI-enabled systems are the most common, while
                XAI-enabled CDSS for text analysis are the least common in
                literature. There is more interest in developers for the
                provision of local explanations, while there was almost a balance
                between post-hoc and ante-hoc explanations, as well as between
                model-specific and model-agnostic techniques. Studies reported
                benefits of the use of XAI such as the fact that it could enhance
                decision confidence for clinicians, or generate the hypothesis
                about causality, which ultimately leads to increased
                trustworthiness and acceptability of the system and potential for
                its incorporation in the clinical workflow. However, we found an
                overall distinct lack of application of XAI in the context of
                CDSS and, in particular, a lack of user studies exploring the
                needs of clinicians. We propose some guidelines for the
                implementation of XAI in CDSS and explore some opportunities,
                challenges, and future research needs.},
    DOI = {10.3390/app11115088},
}
@inproceedings{XAIevalCNN,
    title = {Evaluation and comparison of CNN visual explanations for
             histopathology},
    author = {Graziani, Mara and Lompech, Thomas and M{\"u}ller, Henning and
              Andrearczyk, Vincent},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence
                 Workshops (XAI-AAAI-21), Virtual Event},
    pages = {8--9},
    year = {2021},
}
@misc{XAIOnC,
    title = {Opportunities and Challenges in Explainable Artificial Intelligence
             (XAI): A Survey},
    author = {Arun Das and Paul Rad},
    year = {2020},
    eprint = {2006.11371},
    archivePrefix = {arXiv},
    primaryClass = {id='cs.CV' full_name='Computer Vision and Pattern
                    Recognition' is_active=True alt_name=None in_archive='cs'
                    is_general=False description='Covers image processing,
                    computer vision, pattern recognition, and scene
                    understanding. Roughly includes material in ACM Subject
                    Classes I.2.10, I.4, and I.5.'},
}
@inproceedings{XAIglobal,
    author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley,
              John},
    title = {Global Explanations of Neural Networks: Mapping the Landscape of
             Predictions},
    year = {2019},
    isbn = {9781450363242},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3306618.3314230},
    doi = {10.1145/3306618.3314230},
    abstract = {A barrier to the wider adoption of neural networks is their lack
                of interpretability. While local explanation methods exist for
                one prediction, most global attributions still reduce neural
                network decisions to a single set of features. In response, we
                present an approach for generating global attributions called GAM
                , which explains the landscape of neural network predictions
                across subpopulations. GAM augments global explanations with the
                proportion of samples that each attribution best explains and
                specifies which samples are described by each attribution. Global
                explanations also have tunable granularity to detect more or
                fewer subpopulations. We demonstrate that GAM's global
                explanations 1) yield the known feature importances of simulated
                data, 2) match feature weights of interpretable statistical
                models on real data, and 3) are intuitive to practitioners
                through user studies. With more transparent predictions, GAM can
                help ensure neural network decisions are generated for the right
                reasons.},
    booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and
                 Society},
    pages = {279–287},
    numpages = {9},
    keywords = {neural networks, global interpretability, explainable deep
                learning},
    location = {Honolulu, HI, USA},
    series = {AIES '19},
} 

@misc{LIMEwhy,
    title = {Why model why? Assessing the strengths and limitations of LIME},
    author = {Jürgen Dieber and Sabrina Kirrane},
    year = {2020},
    eprint = {2012.00093},
    archivePrefix = {arXiv},
    primaryClass = {id='cs.LG' full_name='Machine Learning' is_active=True
                    alt_name=None in_archive='cs' is_general=False
                    description='Papers on all aspects of machine learning
                    research (supervised, unsupervised, reinforcement learning,
                    bandit problems, and so on) including also robustness,
                    explanation, fairness, and methodology. cs.LG is also an
                    appropriate primary category for applications of machine
                    learning methods.'},
}
    
@article{SHAPmanip,
    AUTHOR = {Walia, Savita and Kumar, Krishan and Agarwal, Saurabh and Kim,
              Hyunsung},
    TITLE = {Using XAI for Deep Learning-Based Image Manipulation Detection with
             Shapley Additive Explanation},
    JOURNAL = {Symmetry},
    VOLUME = {14},
    YEAR = {2022},
    NUMBER = {8},
    ARTICLE-NUMBER = {1611},
    URL = {https://www.mdpi.com/2073-8994/14/8/1611},
    ISSN = {2073-8994},
    ABSTRACT = {In the arena of image forensics, detecting manipulations in an
                image is extremely significant because of the use of images in
                different fields. Various detection techniques have been
                suggested in the literature that are based on digging out the
                features from images to unveil the traces left by manipulation
                operations. In this paper, a deep learning-based approach is
                proposed in which a residual network is used to learn deep,
                complex features from preprocessed images for classification into
                authentic and forged images. There is statistical symmetry in
                similar types of images and asymmetry in different types of
                images. The proposed scheme can highlight the statistical
                asymmetry between authentic and forged images. In the proposed
                scheme, firstly, an RGB image is analyzed for different JPEG
                compression levels. The obtained difference between the error
                levels is used to extract enhanced LBP code. Then, the scale- and
                direction-invariant LBP (SD-LBP) code is transformed into SD-LBP
                feature maps to feed to a deep residual network. Next, the
                concept of explainable artificial intelligence (XAI) is used to
                help provide explanations and interpret the output, thereby
                raising the credibility of the proposed approach. The unique
                feature selection approach employed is the kernel SHAP method,
                which is focused on the Shapley values. This technique is used to
                pinpoint the specific characteristics that are responsible for
                the aberrant behavior of the forged images dataset. Later, the
                deep learning-based model is trained and validated using these
                feature sets. A pre-activation version of ResNet-50 architecture
                is used that achieved an accuracy of 99.31%, 99.52%, 98.05%, and
                99.10% on CASIA v1, CASIA v2, IMD 2020, and DVMM datasets,
                respectively. The capability of the pretrained residual network
                and rich textural features, which are scale- and
                direction-invariant, helps to expand the detection accuracy of
                the proposed approach. The results confirmed that the method
                either produced competitive results or outperformed existing
                methods.},
    DOI = {10.3390/sym14081611},
}

@article{XAIcounter,
    title = {Explainable image classification with evidence counterfactual},
    author = {Vermeire, Tom and Brughmans, Dieter and Goethals, Sofie and De
              Oliveira, Raphael Mazzine Barbossa and Martens, David},
    journal = {Pattern Analysis and Applications},
    volume = {25},
    number = {2},
    pages = {315--335},
    year = {2022},
    publisher = {Springer},
}

@article{medicalXAIexample,
    author = {Tjoa, Erico and Guan, Cuntai},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    title = {A Survey on Explainable Artificial Intelligence (XAI): Toward
             Medical XAI},
    year = {2021},
    volume = {32},
    number = {11},
    pages = {4793-4813},
    keywords = {Artificial intelligence;Machine learning;Medical information
                systems;Machine learning algorithms;Explainable artificial
                intelligence (XAI);interpretability;machine learning (ML);medical
                information system;survey},
    doi = {10.1109/TNNLS.2020.3027314},
}

@article{medicalXAIexampletret,
    title = {Convolutional neural network based data interpretable framework for
             Alzheimer’s treatment planning},
    author = {Parvin, Sazia and Nimmy, Sonia Farhana and Kamal, Md Sarwar},
    journal = {Visual Computing for Industry, Biomedicine, and Art},
    volume = {7},
    number = {1},
    pages = {1--12},
    year = {2024},
    publisher = {Springer},
}

@inproceedings{qualityXAIexample,
    title = {An interactive explanatory AI system for industrial quality control
             },
    author = {M{\"u}ller, Dennis and M{\"a}rz, Michael and Scheele, Stephan and
              Schmid, Ute},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume = {36},
    number = {11},
    pages = {12580--12586},
    year = {2022},
}

@article{marketingXAIexample,
    title = {Explainable AI: From black box to glass box},
    author = {Rai, Arun},
    journal = {Journal of the Academy of Marketing Science},
    volume = {48},
    pages = {137--141},
    year = {2020},
    publisher = {Springer},
}

@article{lover2021explainable,
    title = {Explainable AI methods on a deep reinforcement learning agent for
             automatic docking},
    author = {L{\o}ver, Jakob and Gj{\ae}rum, Vilde B and Lekkas, Anastasios M},
    journal = {IFAC-PapersOnLine},
    volume = {54},
    number = {16},
    pages = {146--152},
    year = {2021},
    publisher = {Elsevier},
}

@article{imageclassficationchallanges,
    title = {Artificial intelligence-based image classification methods for
             diagnosis of skin cancer: Challenges and opportunities},
    author = {Goyal, Manu and Knackstedt, Thomas and Yan, Shaofeng and
              Hassanpour, Saeed},
    journal = {Computers in biology and medicine},
    volume = {127},
    pages = {104065},
    year = {2020},
    publisher = {Elsevier},
}

@article{nature,
    title = {Deep learning},
    author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    journal = {nature},
    volume = {521},
    number = {7553},
    pages = {436--444},
    year = {2015},
    publisher = {Nature Publishing Group UK London},
}

@manual{python,
    title = {Python},
    author = {Python Software Foundation},
    note = {\url{https://www.python.org/} Accessed: 2024-06-15},
}
@manual{tensorflow,
    title = {TensorFlow},
    note = {\url{https://www.tensorflow.org/} Accessed: 2024-06-15},
}

@manual{limedoc,
    title = {lime},
    author = {Marco Tulio Ribeiro},
    note = {\url{https://lime-ml.readthedocs.io/en/latest/} Accessed: 2024-06-15
            },
}
@manual{shapdoc,
    title = {SHAP},
    author = {Scott Lundberg},
    note = {\url{https://shap.readthedocs.io/en/latest/} Accessed: 2024-06-15},
}
@manual{numpy,
    title = {NumPy},
    author = {NumPy team},
    note = {\url{https://numpy.org/} Accessed: 2024-06-15},
}
@manual{matplotlib,
    title = {Matplotlib},
    author = {The Matplotlib development team},
    note = {\url{https://matplotlib.org/} Accessed: 2024-06-15},
}
@manual{scikitimage,
    title = {scikit-image},
    author = {the scikit-image development team},
    note = {\url{https://scikit-image.org/} Accessed: 2024-06-15},
}
@manual{pandas,
    title = {pandas},
    author = {{NumFOCUS, Inc.}},
    note = {\url{https://pandas.pydata.org/} Accessed: 2024-06-15},
}
@manual{seaborn,
    title = {seaborn},
    author = {Michael Waskom},
    note = {\url{https://seaborn.pydata.org/} Accessed: 2024-06-15},
}
@manual{imagenet,
    title = {ImageNet-9},
    note = {\url{https://github.com/MadryLab/backgrounds_challenge} Accessed:
            2024-06-15},
}
