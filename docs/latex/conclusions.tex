% !TEX encoding = UTF-8 Unicode 
% !TEX root = praca.tex

\chapter*{Wnioski}

W przeprowadzonych badaniach skupiono się na analizie i porównaniu różnych metod wyjaśnialnej sztucznej inteligencji (XAI), w tym LIME, SHAP i GradCAM, pod kątem ich spójności, dokładności oraz wpływu na pewność modelu.
Przeanalizowano również, jak łączenie wyjaśnień tych metod wpływa na jakość i interpretowalność wyników.
Poniżej przedstawiono szczegółowe podsumowanie wyników oraz wnioski z przeprowadzonych badań.

\section*{Spójność obszarów wyjaśnień}

Analiza spójności obszarów wyjaśnień wykazała, że najlepszą spójność osiągano w przypadku połączenia GradCAM z LIME, podczas gdy najgorsze wyniki uzyskano dla połączenia LIME z SHAP.
Oznacza to, że GradCAM i LIME miały większą zgodność co do istotnych obszarów w obrazach, natomiast LIME i SHAP identyfikowały te obszary bardziej rozbieżnie.

W podziale na kategorie również zaobserwowano, że połączenie GradCAM z LIME charakteryzowało się najlepszą spójnością, a najgorsze wyniki uzyskano dla połączenia LIME z SHAP.
Zaobserwowano że kategoria Insekt posiada ponad przeciętną spójność dla wszystkich metod, natomiast Naczelny posiada spójność poniżej typowej dla wszystkich metod.

Podział na rozmiary wykazał, że GradCAM z LIME osiągały najlepszą spójność niezależnie od wielkości obrazu.
Spójność GradCAM z LIME zmniejszała się wraz ze wzrostem wielkości obiektu.
Natomiast spójność GradCAM z SHAP była na podobnym poziomie niezależnie od wielkości obiektu.

\section*{Analiza wielkości wyjaśnień}

Analiza wielkości wyjaśnień wykazała, że GradCAM najlepiej radzi sobie z dopasowywaniem wielkości obszarów wyjaśnienia do wielkości obiektu.
Zauważono również ograniczenie GradCAM spowodowane sposobe obliczania wyjaśnień, rozdzielczość wyjaśnienia jest zależna od wielkości ostatniej warstwy konwolucyjnej modelu.
SHAP wykazał najniższą korelacje lub nawet jej brak, między rozmiarem obszaru wyjaśnienia, faktycznym rozmiarem obiektu na obrazie.

\section*{Analiza porównawcza metod}

Porównując metody XAI pod kątem rzeczywistych wartości za pomocą metryki IoU najlepsze wyniki osiągał GradCAM, wyjątkiem były obiekty Bardzo małe dla których najlepszą metodą okazał się LIME.

Porównikac metody XAI pod kątem średniego procentu obszaru wyjaśnienia poza faktycznym obszarem obiektu najlepsze wyniki osiągnął LIME, co oznacza że wybierał on najżadziej obszary na pewno nie waże dla klasyfikacji.
Następnie był GradCAM, który ponownie był ograniczony wielkością ostatniej warstwy konwolucyjnej modelu, przez co częściej oznaczał obszary nie ważne.

Porównując zmiany pewności najlepiej wyszedł GradCAM.
Zarówno LIME jak i SHAP skupiają się na mniejszych fragmentach obrazu co powoduje, mniejszy wpływ na pewność po modyfikacji obrazów.

\section*{Łączenie wyjaśnień}

Połączenie wyjaśnień przez część wspólną wykazało, że najlepsze wyniki osiągnięto dla połączenia GradCAM z LIME.
Jednakże, niezależnie od kombinacji metod, wyniki te były gorsze od wyników uzyskanych z poszczególnych metod.
Zmiany pewności modelu po pozostawieniu wyjaśnień były największe dla połączenia GradCAM z LIME, lecz nadal gorsze od podstawowych wyjaśnień.

Natomiast połączenie wyjaśnień poprzez sumę obszarów przyniosło lepsze wyniki niż dla pojedynczych metod.
Najlepsze wyniki uzyskano dla połączenia GradCAM z LIME, natomiast najgorsze dla SHAP z LIME.
Wartości IoU były wyższe dla połączonych wyjaśnień, co sugeruje, że takie podejście może zwiększać dokładność identyfikacji istotnych obszarów.
Zmiana pewności modelu po pozostawieniu sumy obszarów wyjaśnień była największa dla połączenia wszystkich trzech metod, co sugeruje, że suma wyjaśnień może dostarczać bardziej kompleksowych informacji.
Jednak zmiana pewności po usunięciu tych obszarów również była największa dla tej kombinacji, co wskazuje na znaczną istotność tych obszarów dla modelu.

\section*{Wnioski końcowe}

Badania wykazały, że GradCAM jest najbardziej spójną i dokładną metodą wyjaśnień w porównaniu do LIME i SHAP.
Łączenie wyjaśnień różnych metod może poprawić dokładność i spójność wyników, szczególnie gdy stosuje się sumę obszarów.
W praktyce, wybór odpowiedniej metody XAI oraz podejście do łączenia wyjaśnień powinno być dostosowane do specyfiki problemu i wymagań interpretowalności modelu.
