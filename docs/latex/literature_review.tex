% !TEX encoding = UTF-8 Unicode 
% !TEX root = praca.tex

\chapter*{Przegląd literatury}
\section*{Metody klasyfikacji obrazów}

Klasyfikacja obrazów to jedno z fundamentalnych zagadnień w dziedzinie sztucznej inteligencji, które ma na celu przypisanie etykiet do obrazów na podstawie ich cech.
Jest to zagadnienie o szerokim zastosowaniu, znajdujące praktyczne zastosowanie w wielu dziedzinach życia, od medycyny i biologii po przemysł, marketing czy bezpieczeństwo.

Jednym z głównych wyzwań w klasyfikacji obrazów jest różnorodność danych i złożoność struktury obrazów.
Obrazy mogą zawierać wiele różnych obiektów, o różnych kształtach, rozmiarach, orientacjach i położeniach, co utrudnia automatyczne rozpoznawanie i klasyfikację.
Ponadto, obrazy mogą być podatne na różne rodzaje zniekształceń, takie jak zmiany oświetlenia, rozmazania czy zakłócenia, co dodatkowo utrudnia proces klasyfikacji.
Negatywny wpływ na wynik klasyfikacji mogą mieć niskiej jakości zbiory danych

Źródłami  problemów mogą być zbiory danych, w których wszystkie dane pochodzą z tego samego lub podobnego źródła, może to skutokować przuczeniem się modelu, poprzez nauczenie się modelu na nieoczekiwanych cech w celu klasyfikacji.

\textbf{Głębokie sieci neuronowe} (DNN) to modele, które składają się z wielu warstw neuronów, umożliwiając automatyczne uczenie się hierarchicznych cech na różnych poziomach abstrakcji, co sprawia, że są one bardzo skuteczne w analizie i rozpoznawaniu wzorców w danych obrazowych.

\textbf{Sieci konwolucyjne} (CNN) są szczególnie skutecznym rodzajami głębokich sieci neuronowych stworzonym do przetwarzania danych obrazowych.
Ich achitektura opiera się na kilku podstawowych rodzajach warstw, które skutecznie wyodrębniają cechy obrazów na różnych poziomach abstrakcji.
Podstawowymi elementami sieci konwolucyjnch są:
\begin{itemize}
	\item \textbf{Warstwy konowlucyjne}, wykorzystują zestaw filtrów, które przesuwają się po obrazie, wykonując operację konwolucji.
	      Ta operacja pozwala na wyodrębnienie lokalnych cech z tego obrazu, takich jak krawędzie, tekstury czy wzory.
	\item \textbf{Warstwy poolingowe}, występują po warstwach konwolucyjnych , redukują wymiary przestrzenne map cech.
\end{itemize}
Sieci konwolucyjne są szczególnie skuteczne w analizie danych obrazowych ze względu na ich zdolność do automatycznego wyodrębniania cech lokalnych i hierarchicznego uczenia się abstrakcji na różnych poziomach.
Ich zastosowanie znalazło szerokie zastosowanie ww rozpoznawaniu obrazów, klasyfikacji, detekcji i segmentaacji obrazów, a także w innych dziedzinach przetwarzania danych, takich jak analiza tekstu czy przetwarzanie języka naturalnego.

\textbf{ResNet} (Residual Neural Network) to innowacyjna architektura głębokiej sieci neuronowej, która została zaprojektowana w celu rozwiązania problemu zanikającego gradientu w głębokich sieciach.
Problem zanikającego gradientu występuje gdy uczenie się modelu staje się trudniejsze wraz ze wzrostem liczby warstw, co prowadzi do ograniczenia wydajności modelu.

Główną cechą ResNet są tzw. bloki rezydualne, które wprowadzają połączenia skrótowe między warstwami, pozwalając na przekazywanie informacji wstecz w sieci.
Dzięki tym połączeniom model może uczyć się reszty funkcji, co eliminuje problem zanikającego gradientu.
Znany jest ze swojej zdolności do efektywnego ucenia się bardzo głębokich modeli, nawet sięgających kilkuset warstw.
Ta cecha sprawia, ze ResNet stał się jedną z najbardziej popularnych architektur w dziedzinie rozpoznawania obrazów, osiągając znakomite wyniki.

\textbf{ResNet50} jest konkretną wersją architektury ResNet, która składa się z 50 warstw.
Jest to jedna z najpopularniejszych wariantów ResNet.

\section*{Metody XAI}
W ostatnich latach głębokie sieci neuronowe (DNN) osiągnęły niezwykłe sukcesy w wielu dziedzinach, takich jak rozpoznawanie obrazów, analiza tekstu czy przetwarzanie języka naturalnego.
Pomimo ich imponującej dokładności predykcji, istnieje coraz większa potrzeba zrozumienia, dlaczego modele te podejmują konkretne decyzje.
W kontekście krytycznych zastosowań, takich jak medycyna czy bezpieczeństwo, zrozumienie powodów, dla których model dokonuje konkretnej predykcji, jest niezwykle istotne.

Wyjaśnialna sztuczna inteligencja (XAI) ma na celu zwiększenie transparentności i zrozumienia procesu podejmowania decyzji przez modele głębokiego uczenia.
Będąc w stanie określić jakie cechy mialy wpływ na predykcję, można ocenić czy nauczył się w odpowiedni sposób, posiadając odpowiednią wiedzę na temat danych treningowych.
Metody XAI pozwalają na analizę i interpretację działania modeli, co umożliwia użytkownikom oraz programistom zrozumienie, dlaczego model dokonał konkretnej predykcji i jakie cechy danych miały największy wpływ na tę decyzję.

W niniejszej części przeglądu literatury skupiono się na omówieniu dwóch głównych grup metod XAI:
\begin{itemize}
	\item \textbf{Metody niezależne od modelu} (Model-agnostic)
	\item \textbf{Metody zależne od modelu} (Model-specific)
\end{itemize}
Metody te różnią się swoim podejściem do wyjaśniania decyzji modeli i mają różne zalety oraz ograniczenia.

Poznanie tych metod, pomaga w lepszym zrozumieć, jakie są różnice między nimi, jak działają i w jaki sposób mogą być stosowane w praktyce.
Można dzięki temu wybrać odpowiednią metodę XAI dla konkretnych zastosowań oraz zwiększając tym zaufanie do modeli głębokiego uczenia poprzez ich lepsze zrozumienie i interpretację.

\subsection*{Model-agnostic}
Metody oparte na podejściu Model-agnostic są technikami XAI, które niezależnie od konkretnego modelu potrafią wyjaśnić jego decyzje.
Charakteryzują się one uniwersalnością i zdolnością do stosowania wobec różnych rodzajów modeli, co czyni je atrakcyjnym narzędziem dla praktyków i badaczy.

Główną zaletą tego podejścia jest jego niezależność od wewnętrznych mechanizmów modelu, co oznacza, że metody te mogą być stosowane do wyjaśniania decyzji zarówno prostych, jak i bardziej skomplikownych modeli, w tym sieci neuronowych czy drzew decyzyjnych.

\subsubsection*{LIME}
\textbf{LIME} \cite{ribeiro2016why} (Local Interpretable Model-agnostic Explanations) to jedna z najpopularniejszych technik XAI, która umożliwia tworzenie lokalnie interpretowalnych modeli wokół wybranych przypadków danych.
Metoda ta polega na generowaniu lokalnych wyjaśnień, które opisują, jakie cechy danych wejściowych wpływają na decyzje modelu.
Poprzez symulowanie zbliżonych instancji danych i analizę reakcji modelu na ich zmiany, LIME pozwala na zrozumienie, które cechy mają największy wpływ na wynik predykcji.



Metoda LIME składa się z następujących kroków:
\begin{enumerate}
	\item \textbf{Wybór instancji do wyjaśnienia} - Na początku wybierana jest konkretna instancja danych, której decyzja modelu zostanie  wyjaśniona.
	\item \textbf{Generowanie perturbacji} - Następnie generowany jest zestaw perturbowanych wersji oryginalnej instancji.
	      Perturbacje te są tworzone przez wprowadzenie małych, losowych zmian do cech wejściowych.
	      W przypdaku danych obrazowych, perturbacje mogą polegać na zmianie wartości pikseli lub zamaskowaniu fragmentów obrazu.
	\item \textbf{Ocena perturbowanych instancji} - Każda perturbowana instancja jest następnie przekazywana do orginalnego model, aby uzyskać predykcję dla nich
	\item \textbf{Ważenie perturbacji} - Perturbacje są ważone na podstawie ich odległości od orginalnej instancji.
	      Perturbacje bliższe oryginalnej instancji mają większy wpływ na trenowanie lokalnego modelu.
	\item \textbf{Tworzenie loalnego modelu} - LIME buduje prosty, lokalny model interpretowalny (np. regresja liniowa) zw oparciu o perturbowane instancje i ich odpowiadające predykcje, z uwzględnieniem ich wag.
	      Ten model jest trenowany, aby najlepiej odwzorować zachowanie oryginalnego modelu w okolicy wybranej instancji.
	\item \textbf{Generowanie wyjaśnień} - Po zbudowaniu lokalnego modelu interpretowalnego, LIME identyfikuje najważniejsze cechy, które wpływają na predykcję oryginalnego modelu dla wybranej instancji.
	      Wyjaśnienia te są przedstawiane w postaci listy cech z przypisanymi wagami, które pokazują ich wpływ na decyzję modelu.
\end{enumerate}

Poniżej znajduje się pseudokod prezentujący kroki algorytmu LIME:
\begin{listing}
	\begin{minted}{python}
        def explain_with_lime(instance):
            perturbed_instances = generate_perturbations(instance)
            predictions = model.predict(perturbed_instances)
            weights = compute_weights(perturbed_instances, instance)
            local_model = train_local_model(perturbed_instances, predictions, weights)
            explanations = generate_explanations(local_model)
            return explanations
  \end{minted}
	\caption{Pseudo kod LIME} \label{listing:lime}
\end{listing}

LIME jest techniką XAI, która pozwala na zrozumienie, jakie cechy danych wpływają na decyzje modelu, konkretnych instancji danych.
Dzięki LIME, użytkownicy mogą lepiej zrozumieć, dlaczego model podjął konkretną decyzję.
W przypadku danych obrazowych LIME umożliwia wizualizację wpływu poszczególnych fragmentów obrazu na decyzje modelu, co dodatkowo zwiększa interpretowalność i zrozumienie modelu.

\subsubsection*{SHAP}
\textbf{SHAP} \cite{lundberg2017unified} (SHapley Additive exPlanations) wykorzystuje teorię gier do przypisywania znaczenia poszczególnym cechom wejściowym w procesie decyzyjnym modelu.
Metoda ta opiera się na szacowaniu wartości Shapleya, które określają, jaki wpływ ma każda cecha na przewidywaną wartość.
Dzięki tej technice możemy zidentyfikować, które cechy mają największy wpływ na wynik modelu i jakie są ich wzajemne zależności.

Teoria wartości Shapleya pochodzi z teori gier kooperacyjnych i służą do podziału zysków (lub wpływów) między graczy (cechy w kontekście modelu) proporcjonalnie do ich wkładu w wynik.
Dla każdej cechy wartość Shapleya obliczana jest jako średni wkład tej cechy do wyniku modelu we wszystkich możliwych kombinacjach cech.

Metoda SHAP składa się z następujących kroków:
\begin{enumerate}
	\item \textbf{Wybór instancji}
	\item \textbf{Perturbacja danych} - Dla każdej cechy generowane są wszystkie możliwe podzbiory cech (kombinacje cech z wyjątkiem tej wybranej)
	\item \textbf{Ocena wpływu cech} - Model jest uruchamiany dla każdego podzbioru cech z i bez wybranej cechy, a różnice w wynikach są mierzone
	\item \textbf{Średnia marginalna wartości} - Wartości Shapleya dla danej cechy są obliczane jako średnia marginalnego wkładu tej cechy do wyniku modelu we weszystkich możliwych podzbiorach cech
\end{enumerate}

Obliczanie dokładnych wartości Shapleya może być obliczeniowo kosztowne, dlatego SHAP wykorzystuje różne techniki przyspieszanie obliczeń, takie jak aproksymacje i metody samplingowe.

SHAP to technika XAI, która pozwala na precyzyjne przypisywanie znaczenia poszczególnym cechom wejściowym.
Dzięki wykorzystaniu teori wartości Shapleya, SHAP zapewnia spójne wyjaśnienia, co jest niezwykle cenne w zrozumieniu działania modeli głębokiego uczenia.
Jest jednak kosztowny obliczeniowo.
W przypadku danych obrazowych, SHAP umożliwia identyfikację wpływu poszczególnych super pikseli na predykcję modelu co jest kluczowe dla uzyskania wglądu w mechanizmy decyzyjne modelu.

\vspace{1cm}
Podsumowując, \textbf{Metody Model-agnostic} oferują elastyczne i uniwersalne podejście do wyjaśniania decyzji modeli, co czyni je atrakcyjnym narzędziem dla szerokiego spektrum zastosowań.
Ich zdolność do pracy z różnymi rodzajami modeli oraz możliwość generowania lokalnych wyjaśnień pozwala na lepsze zrozumienie i interpretację działania modeli głębokiego uczenia.

\subsection*{Oparte na podejściu Model-specific}
Metody oparte na podejściu Model-specific to techniki XAI, które są związane bezpośrednio z architekturą i działaniem konkretnego modelu.
Charakteryzują się one wysokim stopniem specjalizacji i dostosowania do konkretnych rodzajów modeli, co umożliwia uzyskanie bardziej szczegółowych i dokładnych wyjaśnień ich działania oraz mniejsze koszty obliczeniowe.

W odróżnieniu od metod niezależnych od modelu, metody te wykorzystują wewnętrzną strukturę i parametry modelu, aby dostarczyć wyjaśnień dotyczących podejmowanych decyzji.
Dzięki temu są w stanie dokładniej wskazać, które elementy modelu oraz które cechy mają kluczowy wpływ na wynik predykcji.

% GradCAM
\textbf{GradCAM} \cite{Selvaraju_2019} (Gradient-weighted Class Activation Mapping) jest jedną z najbardziej popularnych metod model-specific, która umożliwia wizualizację obszarów obrazu, które najbardziej wpływają na decyzję klasyfikacji modelu.
Metoda ta wykorzystuje gradienty wsteczne, aby obliczyć istotność poszczególnych super pikseli obrazu względem konkretnej klasy.
Dzięki temu możemy zidentyfikować, które obszary obrazu były decydujące dla klasyfikacji i w jaki sposób model dokonywał swoich predykcji.

GradCAM składa się z następujących kroków:
\begin{itemize}
	\item \textbf{Forward pass przez model} - przeprowadzamy forward pass przez model, aby uzyskać mapy cech i predykcje.
	\item \textbf{Obliczenie gradientów} - przeprowadzamy backward pass, aby uzyskać gradient klasy docelowej względem map cech.
	\item \textbf{Uśrednienie gradientów} - wykonujemy global average pooling, aby uzyskać wagi istotności.
	\item \textbf{Ważona suma map cech} - wykonujemy ważoną sumę map cech, wykorzystując wagi istotności.
	\item \textbf{ReLU} - zastosowanie funkcji ReLU do ważonej sumy, aby uzyskać ostateczną mapę cieplną.
	\item \textbf{Normalizacja} - Normalizujemy mapę cieplną do zakresu [0, 1].
\end{itemize}

Poniżej znajduje się pseudo kod który wizualizuje kroki algorytmu GradCAM:

\begin{listing}
	\begin{minted}{python}
features = model.forward(image)
predictions = model.predict(features)
gradients = model.backward(class_idx, features)
weights = mean(gradients, axis=(0, 1))
cam = zeros(features.shape[1:3])
for i in range(len(weights)):
    cam += weights[i] * features[i]

cam = maximum(cam, 0)
cam = cam / max(cam)
    \end{minted}
	\caption{Pseudo kod GradCAM} \label{listing:gradcam}
\end{listing}

GradCAM dostarcza  wizualnych wyjaśnień, które są intuicyjne do interpretacji.
Dzięki mapom ciepła (heatmaps) generowanym przez tę metodę, możemy zobaczyć, które regiony obrazu najbardziej wpłynęły na predykcję modelu bez konieczności posiadania wiedzy ekspertskiej.

\vspace{1cm}

Podsumowując, \textbf{Metody Model-specific} oferują wysoki poziom szczegółowości i precyzji w wyjaśnianiu działania konkretnych modeli, co czyni je atrakcyjnym narzędziem dla zrozumienia ich wewnętrznej logiki i mechanizmów decyzyjnych.
Ich dostosowanie do konkretnych architektur modeli pozwala na uzyskanie bardziej precyzyjnych i zrozumiałych wyjaśnień, co jest kluczowe w przypadku zastosowań wymagających wysokiego stopnia pewności i zaufania do modeli.
Dzięki temu metody takie jak GradCAM mogą nie tylko zwiększać zaufanie użytkowników do modeli, ale także prowadzić do ich dalszej optymalizacji i poprawy wyników.

\section*{Porównywanie metod XAI w klasyfikacji obrazów}
%Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing
%DHIS - Evaluation of Explainable Artificial Intelligence: SHAP, LIME, and CAM
%Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization
%CLEVER_XAI
Porównywanie metod wyjaśnialnej sztucznej inteligencji (XAI) w kontekście klasyfikacj obrazów jest kluczowe, aby zrozumieć, które techniki są najbardziej skuteczne i jakie mają zalety oraz ograniczenia.
W pracy 'Ablation-CAM' \cite{9093360} zaprezentowano dwa podejścia do porównywania metod XAI: oparte na modelu (model-based) oraz oparte na prawdzie (truth-based).
\begin{itemize}
	\item \textbf{Oparte na modelu} (model-based)
	\item \textbf{Oparte na prawdzie} (truth-based)
\end{itemize}

\subsection*{Oparte na modelu}
Metody oparte na modelu oceniają wyjaśnienia XAI poprzez analizie ich wpływu na wyniki modelu predykcyjnego.
Przykładowe metryki stosowane w tym podejściu:
\begin{itemize}
	\item \textbf{Średni spadek pewności i wyniku aktywacji} (Average drop in confidence and activation score) - ta metryka mierzy średni spadek pewności modelu (\ref{eq:average_drop}) w przypadku dostarczenia jedynie obszarów, które zostały zidentyfikowane jako ważne przez metodę XAI.
	      Jeśli metoda XAI poprawnie identyikuje istotne cechy, powinno to prowadzić do niewielkiego spadku pewności predykcji modelu.
	      Metryka ta jest używana do oceny, jak skutecznie metoda XAI identyfikuje krytyczne cechy wpływające na decyzję modelu.
	      \begin{equation}
		      \text{Średni spadek} = \frac{1}{N} \sum_{i=1}^{N} \frac{\max(0,Y_i^c-O_i^c)}{T_i^c} \times 100
		      \label{eq:average_drop}
	      \end{equation}
	      Gdzie:
	      \begin{itemize}[label=]
		      \item $Y_i^c$ to wynik dla orginalnego obrazu
		      \item $O_i^c$ to wynik dla samego obszaru wyjaśnienia
		      \item $N$ to liczba obrazów
	      \end{itemize}
	\item \textbf{Procentowy wzrost pewności i wyniku aktywacji} (Percent increase in confidence and activation score) - ta metryka mierzy procent przypadków wzrost pewności modelu (\ref{eq:rate_of-increase}), gdy tylko cechy zidentyfikowane jako ważne przez metodę XAI pozostają w obrazie, a wszystkie inne cechy są usunięte.
	      Jeśli metoda XAI prawidłowo identyfikuje istotne cechy, pozostawienie tylko tych cech powinno prowadzić do częstego wzrostu pewności predykcji modelu.
	      Ta metryka pozwala ocenić, w jakim stopniu metoda XAI może poprawić zrozumienie krytycznych cech przez model.
	      \begin{equation}
		      \text{Częstotliwość wzrostu pewności} =  \sum_{i=1}^{N} \frac{1_{Y_i^c<O_i^c}}{N} \times 100
		      \label{eq:rate_of-increase}
	      \end{equation}
	      Gdzie:
	      \begin{itemize}[label=]
		      \item $Y_i^c$ to wynik dla orginalnego obrazu
		      \item $O_i^c$ to wynik dla samego obszaru wyjaśnienia
		      \item $N$ to liczba obrazów
	      \end{itemize}
\end{itemize}

\subsection*{Metody oparte na prawdzie}
Metody oparte na prawdzie oceniają wyjaśnienia XAI przez porównanie ich z zewnętrznymi, zdefinowanymi prawdziwymi danymi. Jedną z głównych metryk w tym podjściu jest:
\begin{itemize}
	\item Intersection over Union (IoU) - jest to miara pokrycia między wyjaśnieniami wygenerowanymi przez metodę XAI a zdefiniowanymi obszarami referencyjnymi w obrazie (\ref{eq:iou}), które są uważane za istotne (np. ręcznie oznaczone przez ekspertów).
	      IoU jest obliczane jako stosunek powierzchni przecięcia do powierzchni sumy obszarów wyjaśnień i referencji. Wysokie IoU wskazuje na dobrą zgodność wyjaśnień XAI z rzeczywistością, co oznacza, że metoda XAI skutrcznie identyfikuje istotne cechy.
	      \begin{equation}
		      \text{IoU} = \frac{\text{I}}{\text{U}}
		      \label{eq:iou}
	      \end{equation}
	      Gdzie:
	      \begin{itemize}
		      \item I to powierzchnia przecięcia obszarów
		      \item U to powierzchnia sumy obszarów
	      \end{itemize}
\end{itemize}
