% !TEX encoding = UTF-8 Unicode 
% !TEX root = praca.tex

\chapter*{Przegląd literatury}
\section*{Metody klasyfikacji obrazów}

Klasyfikacja obrazów to jedno z fundamentalnych zagadnień w dziedzinie sztucznej inteligencji, które ma na celu przypisanie etykiet klas do obrazów na podstawie ich cech.
Jest to zagadnienie o szerokim zastosowaniu, znajdujące praktyczne zastosowanie w wielu dziedzinach życia, od medycyny\cite{medical} i biologii\cite{biology} po przemysł, marketing\cite{marketing} czy bezpieczeństwo\cite{security}.

Jednym z głównych wyzwań w klasyfikacji obrazów\cite{imageclassificationchallanges} jest różnorodność danych i złożoność struktury obrazów.
Obrazy mogą zawierać wiele różnych obiektów, o różnych kształtach, rozmiarach, orientacjach i położeniach, co utrudnia automatyczne rozpoznawanie i klasyfikację.
Ponadto, obrazy mogą być podatne na różne rodzaje zniekształceń, takie jak zmiany oświetlenia, rozmazania czy zakłócenia, co dodatkowo utrudnia proces klasyfikacji.
Negatywny wpływ na wynik klasyfikacji mogą mieć niskiej jakości zbiory danych

Źródłami  problemów mogą być zbiory danych, w których wszystkie dane pochodzą z tego samego lub podobnego źródła, co może skutkować przeuczeniem się modelu, poprzez nauczenie się modelu na nieoczekiwanych cech w celu klasyfikacji.

\textbf{Głębokie sieci neuronowe (DNN)}\cite{nature} to modele, które składają się z wielu warstw neuronów, umożliwiając automatyczne uczenie się hierarchicznych cech na różnych poziomach abstrakcji, co sprawia, że są one bardzo skuteczne w analizie i rozpoznawaniu wzorców w danych obrazowych.

Istnieje wiele różnych architektur głębokich sieci neuronowych.
Przykładem są \textbf{sieci konwolucyjne (CNN)}\cite{8379889}, które zostały stworzone do przetwarzania obrazów.
Ich architektura opiera się na kilku podstawowych rodzajach warstw, które skutecznie wyodrębniają cechy obrazów na różnych poziomach abstrakcji.
Podstawowymi elementami sieci konwolucyjnych są:
\begin{itemize}
	\item \textbf{Warstwy konwolucyjne}, wykorzystują zestaw filtrów, które przesuwają się po obrazie, wykonując operację konwolucji.
	      Ta operacja pozwala na wyodrębnienie lokalnych cech z tego obrazu, takich jak krawędzie, tekstury czy wzory.
	      Tworząc nowe obrazy zwane mapami konwolucji.
	\item \textbf{Warstwy poolingowe}, występują po warstwach konwolucyjnych , redukują wymiary przestrzenne map cech.
\end{itemize}
Sieci konwolucyjne są szczególnie skuteczne w analizie danych obrazowych ze względu na ich zdolność do automatycznego wyodrębniania cech lokalnych i hierarchicznego uczenia się abstrakcji na różnych poziomach.
Ich zastosowanie znalazło szerokie zastosowanie ww rozpoznawaniu obrazów, klasyfikacji, detekcji i segmentacji obrazów, a także w innych dziedzinach przetwarzania danych, takich jak analiza tekstu czy przetwarzanie języka naturalnego.

\textbf{ResNet (Residual Neural Network)}\cite{He_2016_CVPR} to innowacyjna architektura głębokiej sieci neuronowej, która została zaprojektowana w celu rozwiązania problemu zanikającego gradientu w głębokich sieciach.
Problem zanikającego gradientu występuje gdy uczenie się modelu staje się trudniejsze wraz ze wzrostem liczby warstw, co prowadzi do ograniczenia wydajności modelu.

Główną cechą ResNet są tzw. bloki rezydualne, które wprowadzają połączenia skrótowe między warstwami, pozwalając na przekazywanie informacji wstecz w sieci.
Dzięki tym połączeniom model może uczyć się reszty funkcji, co eliminuje problem zanikającego gradientu.
ResNet znany jest ze swojej zdolności do efektywnego uczenia się bardzo głębokich modeli, nawet sięgających kilkuset warstw.
Ta cecha sprawia, ze ResNet stał się jedną z najbardziej popularnych architektur w dziedzinie rozpoznawania obrazów, osiągając znakomite wyniki.

\textbf{ResNet50}\cite{Koonce2021} jest konkretną wersją architektury ResNet, która składa się z 50 warstw.
Jest to jedna z najpopularniejszych wariantów ResNet często używaną podczas porównywania wyników, jako punkt odniesienia.

%\section*{Zastosowania klasyfikacji obrazów w różnych dziedzinach}

\section*{Metody XAI}
W ostatnich latach głębokie sieci neuronowe (DNN) osiągnęły niezwykłe sukcesy w wielu dziedzinach, takich jak rozpoznawanie obrazów, analiza tekstu czy przetwarzanie języka naturalnego.
Pomimo ich imponującej dokładności predykcji, istnieje coraz większa potrzeba zrozumienia, dlaczego modele te podejmują konkretne decyzje.
W szczególności w przypadku krytycznych zastosowań, takich jak medycyna czy bezpieczeństwo, zrozumienie powodów, dla których model dokonuje konkretnej predykcji, jest niezwykle istotne.

Wyjaśnialna sztuczna inteligencja (XAI)\cite{XAItax, XAIcurrent, XAIOnC, XAIcounter} ma na celu zwiększenie transparentności i zrozumienia procesu podejmowania decyzji przez modele głębokiego uczenia.
Posiadając wiedzę na temat danych treningowych oraz będąc w stanie określić jakie cechy miały wpływ na predykcję, można ocenić czy nauczył się w odpowiedni sposób.
Metody XAI pozwalają na analizę i interpretację działania modeli, co umożliwia użytkownikom oraz programistom zrozumienie, dlaczego model dokonał konkretnej predykcji i które cechy danych miały największy wpływ na tę decyzję.

Istnieją różnorodne metody wyjaśnialnej sztucznej inteligencji, można je grupować na wiele różnych sposobów w celu lepszego zrozumienia ich działania.
Jednym z nich to podział na wyjaśnienia lokalne\cite{ribeiro2016why} i wyjaśnienia globalne\cite{XAIglobal}.
Wyjaśnienia globalne przedstawiają działanie całego model, natomiast wyjaśnienia lokalne skupiają się na wyjaśnianiu decyzji dla poszczególnych instancji danych.
W problemie klasyfikacji danych używane są zazwyczaj lokalne wyjaśnienia.

Innym sposobem na grupowania XAI jest podział na dwie grupy:
\begin{itemize}
	\item \textbf{Metody niezależne od modelu} (Model-agnostic)
	\item \textbf{Metody zależne od modelu} (Model-specific)
\end{itemize}
Metody te różnią się swoim podejściem do wyznaczania wyjaśnień decyzji modeli i mają różne zalety oraz ograniczenia.

Poznanie zalet i ograniczeń tych metod, pomaga w lepszym zrozumieć, jakie są różnice między nimi, jak działają i w jaki sposób mogą być stosowane w praktyce.
Można dzięki temu wybrać odpowiednią metodę XAI dla konkretnych zastosowań oraz zwiększając tym zaufanie do modeli głębokiego uczenia poprzez ich lepsze zrozumienie i interpretację.

\subsection*{Metody niezależne od modelu}
Metody oparte na podejściu Model-agnostic są technikami XAI, które niezależnie od konkretnego modelu potrafią wyjaśnić jego decyzje.
Charakteryzują się one uniwersalnością i zdolnością do stosowania wobec różnych rodzajów modeli, co czyni je atrakcyjnym narzędziem dla praktyków i badaczy.

Główną zaletą tego podejścia jest jego niezależność od wewnętrznych mechanizmów modelu, co oznacza, że metody te mogą być stosowane do wyjaśniania decyzji zarówno prostych, jak i bardziej skomplikowanych modeli, w tym sieci neuronowych czy drzew decyzyjnych.

Przykładem takiej techniki jest \textbf{LIME} (Local Interpretable Model-agnostic Explanations)\cite{ribeiro2016why, LIMEwhy}, jedna z najpopularniejszych technik XAI, która umożliwia tworzenie lokalnie interpretowalnych modeli wokół wybranych przypadków danych.
Metoda ta polega na generowaniu lokalnych wyjaśnień, które opisują, jakie cechy danych wejściowych wpływają na decyzje modelu.
Poprzez symulowanie zbliżonych instancji danych i analizę reakcji modelu na ich zmiany, LIME pozwala na zrozumienie, które cechy mają największy wpływ na wynik predykcji.

Metoda LIME składa się z następujących kroków:
\begin{enumerate}
	\item \textbf{Wybór instancji do wyjaśnienia} - Na początku wybierana jest konkretna instancja danych, której decyzja modelu zostanie  wyjaśniona.
	\item \textbf{Generowanie perturbacji} - Następnie generowany jest zestaw perturbowanych wersji oryginalnej instancji.
	      Perturbacje te są tworzone przez wprowadzenie małych, losowych zmian do cech wejściowych.
	      W przypadku danych obrazowych, perturbacje mogą polegać na zmianie wartości super-pikseli.
	\item \textbf{Ocena perturbowanych instancji} - Każda perturbowana instancja jest następnie przekazywana do oryginalnego model, aby uzyskać predykcję dla nich
	\item \textbf{Ważenie perturbacji} - Perturbacje są ważone na podstawie ich odległości od oryginalnej instancji.
	      Perturbacje bliższe oryginalnej instancji mają większy wpływ na trenowanie lokalnego modelu.
	\item \textbf{Tworzenie lokalnego modelu} - LIME buduje prosty, lokalny model interpretowalny w oparciu o perturbowane instancje i ich odpowiadające predykcje, z uwzględnieniem ich wag.
	      Ten model jest trenowany, aby najlepiej odwzorować zachowanie oryginalnego modelu w okolicy wybranej instancji.
	\item \textbf{Generowanie wyjaśnień} - Po zbudowaniu lokalnego modelu interpretowalnego, LIME identyfikuje najważniejsze cechy, które wpływają na predykcję oryginalnego modelu dla wybranej instancji.
	      Wyjaśnienia te są przedstawiane w postaci listy cech z przypisanymi wagami, które pokazują ich wpływ na decyzję modelu.
\end{enumerate}

Poniżej znajduje się pseudokod prezentujący kroki algorytmu LIME:
\begin{listing}
	\begin{minted}{python}
        def explain_with_lime(instance):
            perturbed_instances = generate_perturbations(instance)
            predictions = model.predict(perturbed_instances)
            weights = compute_weights(perturbed_instances, instance)
            local_model = train_local_model(perturbed_instances, predictions, weights)
            explanations = generate_explanations(local_model)
            return explanations
  \end{minted}
	\caption{Pseudo kod LIME} \label{listing:lime}
\end{listing}

LIME jest techniką XAI, która pozwala na zrozumienie, jakie cechy danych wpływają na decyzje modelu, konkretnych instancji danych.
Dzięki LIME, użytkownicy mogą lepiej zrozumieć, dlaczego model podjął konkretną decyzję.
W przypadku danych obrazowych LIME umożliwia wizualizację wpływu poszczególnych fragmentów obrazu na decyzje modelu, co dodatkowo zwiększa interpretowalność i zrozumienie modelu.

Innym przykładem techniki lokalnej jest \textbf{SHAP} (SHapley Additive exPlanations)\cite{lundberg2017unified, SHAPmanip} wykorzystuje teorię gier do przypisywania znaczenia poszczególnym cechom wejściowym w procesie decyzyjnym modelu.
Metoda ta opiera się na szacowaniu wartości Shapleya, które określają, jaki wpływ ma każda cecha na przewidywaną wartość.
Dzięki tej technice możemy zidentyfikować, które cechy mają największy wpływ na wynik modelu i jakie są ich wzajemne zależności.

Teoria wartości Shapleya pochodzi z teorii gier kooperacyjnych i służą do podziału zysków między graczy (cechy w kontekście modelu) proporcjonalnie do ich wkładu w wynik.
Dla każdej cechy wartość Shapleya obliczana jest jako średni wkład tej cechy do wyniku modelu we wszystkich możliwych kombinacjach cech.

Metoda SHAP składa się z następujących kroków:
\begin{enumerate}
	\item \textbf{Wybór instancji}
	\item \textbf{Perturbacja danych} - Dla każdej cechy generowane są wszystkie możliwe podzbiory cech
	\item \textbf{Ocena wpływu cech} - Model jest uruchamiany dla każdego podzbioru cech, a różnice w wynikach są mierzone
	\item \textbf{Średnia wartości} - Wartości Shapleya dla danej cechy są obliczane jako średnia marginalnego wkładu tej cechy do wyniku modelu we wszystkich możliwych podzbiorach cech
\end{enumerate}

Obliczanie dokładnych wartości Shapleya może być obliczeniowo kosztowne, dlatego SHAP wykorzystuje różne techniki przyspieszanie obliczeń, takie jak aproksymacje i metody samplingowe.

SHAP to technika XAI, która pozwala na precyzyjne przypisywanie znaczenia poszczególnym cechom wejściowym.
W przypadku danych obrazowych, SHAP umożliwia identyfikację wpływu poszczególnych super-pikseli na predykcję modelu co jest kluczowe dla uzyskania wglądu w mechanizmy decyzyjne modelu.

\vspace{1cm}
Podsumowując, metody model-agnostic oferują elastyczne i uniwersalne podejście do wyjaśniania decyzji modeli, co czyni je atrakcyjnym narzędziem dla różnorodnych zastosowań.
Ich zdolność do pracy z różnymi rodzajami modeli oraz możliwość generowania lokalnych wyjaśnień pozwala na lepsze zrozumienie i interpretację działania modeli głębokiego uczenia.

\subsection*{Metody zależne od modelu}
Metody oparte na podejściu model-specific to techniki XAI, które są związane bezpośrednio z architekturą i działaniem konkretnego modelu.
Charakteryzują się one wysokim stopniem specjalizacji i dostosowania do konkretnych rodzajów modeli, co umożliwia uzyskanie bardziej szczegółowych i dokładnych wyjaśnień ich działania oraz mniejsze koszty obliczeniowe.

W odróżnieniu od metod niezależnych od modelu, metody te wykorzystują wewnętrzną strukturę i parametry modelu, aby dostarczyć wyjaśnień dotyczących podejmowanych decyzji.
Dzięki temu są w stanie dokładniej wskazać, które elementy modelu oraz które cechy mają kluczowy wpływ na wynik predykcji.

\textbf{GradCAM} \cite{Selvaraju_2019} (Gradient-weighted Class Activation Mapping) jest jedną z najbardziej popularnych metod model-specific, która umożliwia wizualizację obszarów obrazu, które najbardziej wpływają na decyzję klasyfikacji modelu.
Metoda ta wykorzystuje gradienty wsteczne, aby obliczyć istotność poszczególnych super-pikseli obrazu względem konkretnej klasy oraz warstwę konwolucyjną sieci CNN w celu wyznaczenia obszarów super-pikseli.
Dzięki temu możemy zidentyfikować, które obszary obrazu były decydujące dla klasyfikacji i w jaki sposób model dokonywał swoich predykcji.

GradCAM składa się z następujących kroków:
\begin{enumerate}
	\item \textbf{Forward pass} - przeprowadzamy forward pass przez model, aby uzyskać mapy cech i predykcje.
	\item \textbf{Obliczenie gradientów} - przeprowadzamy backward pass, aby uzyskać gradient klasy docelowej względem map cech.
	\item \textbf{Uśrednienie gradientów} - wykonujemy global average pooling, aby uzyskać wagi istotności.
	\item \textbf{Ważona suma map cech} - wykonujemy ważoną sumę map cech, wykorzystując wagi istotności.
	\item \textbf{ReLU} - zastosowanie funkcji ReLU do ważonej sumy, aby uzyskać ostateczną mapę cieplną.
	\item \textbf{Normalizacja} - Normalizujemy mapę cieplną do zakresu [0, 1].
  \item \textbf{Wizualizacja} - Stworzoną mapę cieplną nakładamy na obraz w celu wizualizacji.
\end{enumerate}

Poniżej znajduje się pseudo kod który wizualizuje kroki algorytmu GradCAM:

\begin{listing}
	\begin{minted}{python}
features = model.forward(image)
predictions = model.predict(features)
gradients = model.backward(class_idx, features)
weights = mean(gradients, axis=(0, 1))
cam = zeros(features.shape[1:3])
for i in range(len(weights)):
    gradcam += weights[i] * features[i]

gradcam = maximum(cam, 0)
gradcam = gradcam / max(gradcam)
    \end{minted}
	\caption{Pseudo kod GradCAM} \label{listing:gradcam}
\end{listing}

GradCAM dostarcza  wizualnych wyjaśnień, które są intuicyjne do interpretacji.
Dzięki mapom ciepła (heatmaps) generowanym przez tę metodę, możemy zobaczyć, które regiony obrazu najbardziej wpłynęły na predykcję modelu bez konieczności posiadania wiedzy eksperckiej.

\vspace{1cm}

Podsumowując, \textbf{Metody Model-specific} oferują wysoki poziom szczegółowości i precyzji w wyjaśnianiu działania konkretnych modeli, co czyni je atrakcyjnym narzędziem dla zrozumienia ich wewnętrznej logiki i mechanizmów decyzyjnych.
Ich dostosowanie do konkretnych architektur modeli pozwala na uzyskanie bardziej precyzyjnych i zrozumiałych wyjaśnień, co jest kluczowe w przypadku zastosowań wymagających wysokiego stopnia pewności i zaufania do modeli.
Dzięki temu metody takie jak GradCAM mogą nie tylko zwiększać zaufanie użytkowników do modeli, ale także prowadzić do ich dalszej optymalizacji i poprawy wyników.

\section*{Porównywanie metod XAI w klasyfikacji obrazów}
%Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing
%DHIS - Evaluation of Explainable Artificial Intelligence: SHAP, LIME, and CAM
%Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization
%CLEVER_XAI
Porównywanie metod wyjaśnialnej sztucznej inteligencji (XAI) w kontekście klasyfikacji obrazów jest kluczowe, aby zrozumieć, które techniki są najbardziej skuteczne i jakie mają zalety oraz ograniczenia.
W pracy 'Ablation-CAM'\cite{9093360} porównano GradCAM z autorskim rozwiązaniem.
Aby porównać obie metody zaproponowano dwa podejścia do porównywania metod XAI: oparte na modelu (model-based) oraz oparte na prawdzie (truth-based).
\begin{itemize}
	\item \textbf{Oparte na modelu} (model-based)
	\item \textbf{Oparte na prawdzie} (truth-based)
\end{itemize}

\subsection*{Oparte na modelu}
Metody oparte na modelu oceniają wyjaśnienia XAI poprzez analizie ich wpływu na wyniki modelu predykcyjnego.
Przykładowe metryki stosowane w tym podejściu:
\begin{itemize}
	\item \textbf{Średni spadek pewności i wyniku aktywacji}\cite{9093360} (Average drop in confidence and activation score) - ta metryka mierzy średni spadek pewności modelu (\ref{eq:average_drop}) w przypadku dostarczenia jedynie obszarów, które zostały zidentyfikowane jako ważne przez metodę XAI.
	      Jeśli metoda XAI poprawnie identyfikuje istotne cechy, powinno to prowadzić do niewielkiego spadku pewności predykcji modelu.
	      Metryka ta jest używana do oceny, jak skutecznie metoda XAI identyfikuje krytyczne cechy wpływające na decyzję modelu.
	      \begin{equation}
		      \text{Średni spadek} = \frac{1}{N} \sum_{i=1}^{N} \frac{\max(0,Y_i^c-O_i^c)}{T_i^c} \times 100
		      \label{eq:average_drop}
	      \end{equation}
	      Gdzie:
	      \begin{itemize}[label=]
		      \item $Y_i^c$ to wynik dla oryginalnego obrazu
		      \item $O_i^c$ to wynik dla samego obszaru wyjaśnienia
		      \item $N$ to liczba obrazów
	      \end{itemize}
	\item \textbf{Procentowy wzrost pewności i wyniku aktywacji}\cite{9093360} (Percent increase in confidence and activation score) - ta metryka mierzy procent przypadków wzrost pewności modelu (\ref{eq:rate_of-increase}), gdy tylko cechy zidentyfikowane jako ważne przez metodę XAI pozostają w obrazie, a wszystkie inne cechy są usunięte.
	      Jeśli metoda XAI prawidłowo identyfikuje istotne cechy, pozostawienie tylko tych cech powinno prowadzić do częstego wzrostu pewności predykcji modelu.
	      Ta metryka pozwala ocenić, w jakim stopniu metoda XAI może poprawić zrozumienie krytycznych cech przez model.
	      \begin{equation}
		      \text{Częstotliwość wzrostu pewności} =  \sum_{i=1}^{N} \frac{1_{Y_i^c<O_i^c}}{N} \times 100
		      \label{eq:rate_of-increase}
	      \end{equation}
	      Gdzie:
	      \begin{itemize}[label=]
		      \item $Y_i^c$ to wynik dla oryginalnego obrazu
		      \item $O_i^c$ to wynik dla samego obszaru wyjaśnienia
		      \item $N$ to liczba obrazów
	      \end{itemize}
\end{itemize}

\subsection*{Metody oparte na prawdzie}
Metody oparte na prawdzie oceniają wyjaśnienia XAI przez porównanie ich z zewnętrznymi, zdefiniowanymi prawdziwymi danymi. Jedną z głównych metryk w tym podejściu jest:
\begin{itemize}
	\item Intersection over Union (IoU)\cite{9093360, XAIevalCNN} - jest to miara pokrycia między wyjaśnieniami wygenerowanymi przez metodę XAI a zdefiniowanymi obszarami referencyjnymi w obrazie (\ref{eq:iou}), które są uważane za istotne (np. ręcznie oznaczone przez ekspertów).
	      IoU jest obliczane jako stosunek powierzchni przecięcia do powierzchni sumy obszarów wyjaśnień i referencji. Wysokie IoU wskazuje na dobrą zgodność wyjaśnień XAI z rzeczywistością, co oznacza, że metoda XAI skutecznie identyfikuje istotne cechy.
	      \begin{equation}
		      \text{IoU} = \frac{\text{I}}{\text{U}}
		      \label{eq:iou}
	      \end{equation}
	      Gdzie:
	      \begin{itemize}
		      \item I to powierzchnia przecięcia obszarów
		      \item U to powierzchnia sumy obszarów
	      \end{itemize}
\end{itemize}

%\section*{Zastosowania wyjaśnialnych modeli}
%
%Wyjaśnialne modele sztucznej inteligencji zwiększają transparentność i zrozumienie decyzji podejmowanych przez skomplikowane modele głębokiego uczenia.
%Poniżej przedstawiono praktyczne zastosowania XAI w różnych obszach.
%\subsection*{Medycyna}
%\begin{itemize}
%	\item Diagnostyka medyczna\cite{medicalXAIexample}
%	\item Planowanie leczenia\cite{medicalXAIexampletret}
%\end{itemize}
%\subsection*{Przemysł}
%\begin{itemize}
%	\item Predykcja utrzymania ruchu
%	\item Kontrola jakości\cite{qualityXAIexample}
%\end{itemize}
%\subsection*{Marketing i Reklama}
%\begin{itemize}
%	\item Marketing\cite{marketingXAIexample}
%\end{itemize}
%\subsection*{Automatyzacja i Robotyka}
%\begin{itemize}
%	\item Nawigacja i percepcja robotów\cite{lover2021explainable}
%\end{itemize}
