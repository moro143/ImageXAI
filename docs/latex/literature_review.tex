% !TEX encoding = UTF-8 Unicode 
% !TEX root = praca.tex

\chapter*{Przegląd literatury}
\section*{Metody klasyfikacji obrazów}

% PROBLEM KLASYFIKACJI OBRAZÓW
Klasyfikacja obrazów to jedno z fundamentalnych zagadnień w dziedzinie sztucznej inteligencji, które ma na celu przypisanie etykiet do obrazów na podstawie ich cech.
Jest to zagadnienie o szerokim zastosowaniu, znajdujące praktyczne zastosowanie w wielu dziedzinach życia, od medycyny i biologii po przemysł, marketing czy bezpieczeństwo.

Jednym z głównych wyzwań w klasyfikacji obrazów jest różnorodność danych i złożoność struktury obrazów.
Obrazy mogą zawierać wiele różnych obiektów, o różnych kształtach, rozmiarach, orientacjach i położeniach, co utrudnia automatyczne rozpoznawanie i klasyfikację.
Ponadto, obrazy mogą być podatne na różne rodzaje zniekształceń, takie jak zmiany oświetlenia, rozmazania czy zakłócenia, co dodatkowo utrudnia proces klasyfikacji.
Negatywny wpływ na jakość klasyfikacji mogą mieć zbiory danych, jeśli obrazy pochodzą z tego samego źródła, może to powodować uczenie się modelu nieprawidłowych cech.

Szczególnym źródłem problemów mogą być zbiory danych, jeśli wszystkie dane pochodzą z tego samego źródła, może to powodować przuczenie się modelu, poprzez nauczenie się modelu nieprawidłowych cech w celu klasyfikacji.

Głębokie sieci neuronowe (DNN) to modele, które składają się z wielu warstw neuronów, umożliwiając automatyczne uczenie się hierarchicznych cech na różnych poziomach abstrakcji, co sprawia, że są one bardzo skuteczne w analizie i rozpoznawaniu wzorców w danych obrazowych.

Sieci konwolucyjne (CNN) są szczególnie skutecznym rodzajami głębokich sieci neuronowych stworzonym do przetwarzania danych obrazowych.
Ich achitektura opiera się na kilku podstawowych rodzajach warstw, które skutecznie wyodrębniają cechy obrazów na różnych poziomach abstrakcji.
Podstawowymi elementami sieci konwolucyjnch są:
\begin{itemize}
  \item \textbf{Warstwy konowlucyjne}, wykorzystują zestaw filtrów, które przesuwają się po obrazie, wykonując operację konwolucji.
    Ta operacja pozwala na wyodrębnienie lokalnych cech z tego obrazu, takich jak krawędzie, tekstury czy wzory.
  \item \textbf{Warstwy poolingowe}, występują po warstwach konwolucyjnych , redukują wymiary przestrzenne map cech.
\end{itemize}
Sieci konwolucyjne są szczególnie skuteczne w analizie danych obrazowych ze względu na ich zdolność do automatycznego wyodrębniania cech lokalnych i hierarchicznego uczenia się abstrakcji na różnych poziomach.
Ich zastosowanie znalazło szerokie zastosowanie ww rozpoznawaniu obrazów, klasyfikacji, detekcji i segmentaacji obrazów, a także w innych dziedzinach przetwarzania danych, takich jak analiza tekstu czy przetwarzanie języka naturalnego.

ResNet (Residual Neural Network) to innowacyjna architektura głębokiej sieci neuronowej, która została zaprojektowana w celu rozwiązania problemu zanikającego gradientu w głębokich sieciach.
Problem zanikającego gradientu występuje gdy uczenie się modelu staje się trudniejsze wraz ze wzrostem liczby warstw, co prowadzi do ograniczenia wydajności modelu.

Główną cechą ResNet są tzw. bloki rezydualne, które wprowadzają połączenia skrótowe (shortcut connections) między warstwami, pozwalając na przekazywanie informacji wstecz w sieci.
Dzięki tym połączeniom model może uczyć się reszty (residuum) funkcji, co eliminuje problem zanikającego gradientu.
Znany jest ze swojej zdolności do efektywnego ucenia się bardzo głębokich modeli, nawet sięgających kilkuset warstw.
Ta cecha sprawia, ze ResNet stał się jedną z najbardziej popularnych architektur w dziedzinie rozpoznawania obrazów, osiągając znakomite wyniki.

ResNet50 jest konkretną wersją architektury ResNet, która składa się z 50 warstw.
Jest to jedna z najpopularniejszych wariantów ResNet.

\section*{Metody XAI}
W ostatnich latach głębokie sieci neuronowe (DNN) osiągnęły niezwykłe sukcesy w wielu dziedzinach, takich jak rozpoznawanie obrazów, analiza tekstu czy przetwarzanie języka naturalnego.
Pomimo ich imponującej dokładności predykcji, istnieje coraz większa potrzeba zrozumienia, dlaczego modele te podejmują konkretne decyzje.
W kontekście krytycznych zastosowań, takich jak medycyna czy bezpieczeństwo, zrozumienie powodów, dla których model dokonuje konkretnej predykcji, jest niezwykle istotne.

Właśnie tutaj wchodzi w grę wyjaśnialna sztuczna inteligencja (XAI), która ma na celu zwiększenie transparentności i zrozumienia procesu podejmowania decyzji przez modele głębokiego uczenia.
Będąc w stanie określić jakie cechy mialy wpływ na predykcję, jesteśmy w stanie ocenić czy nauczył się w odpowiedni sposób, mając odpowiednią wiedzę na temat danych treningowych.
Metody XAI pozwalają na analizę i interpretację działania modeli, co umożliwia użytkownikom oraz programistom zrozumienie, dlaczego model dokonał konkretnej predykcji i jakie cechy danych miały największy wpływ na tę decyzję.

W niniejszej części przeglądu literatury skupimy się na omówieniu dwóch głównych grup metod XAI: 
\begin{itemize}
  \item Metody niezależne od modelu (Model-agnostic)
  \item Metody zależne od modelu (Model-specific)
\end{itemize}
Metody te różnią się swoim podejściem do wyjaśniania decyzji modeli i mają różne zalety oraz ograniczenia.

Poprzez zgłębienie tych metod, będziemy mogli lepiej zrozumieć, jakie są różnice między nimi, jak działają i w jaki sposób mogą być stosowane w praktyce.
Dzięki temu będziemy w stanie wybrać odpowiednią metodę XAI dla konkretnych zastosowań oraz zwiększyć zaufanie do modeli głębokiego uczenia poprzez ich lepsze zrozumienie i interpretację.

\subsection*{Model-agnostic}
Metody oparte na podejściu Model-agnostic są technikami XAI, które niezależnie od konkretnego modelu potrafią wyjaśnić jego decyzje.
Charakteryzują się one uniwersalnością i zdolnością do stosowania wobec różnych rodzajów modeli, co czyni je atrakcyjnym narzędziem dla praktyków i badaczy.

Główną zaletą tego podejścia jest jego niezależność od wewnętrznych mechanizmów modelu, co oznacza, że metody te mogą być stosowane do wyjaśniania decyzji zarówno prostych, jak i bardziej skomplikownych modeli, w tym sieci neuronowych czy drzew decyzyjnych.

\subsubsection*{LIME}
\textbf{LIME} (Local Interpretable Model-agnostic Explanations) to jedna z najpopularniejszych technik XAI, która umożliwia tworzenie lokalnie interpretowalnych modeli wokół wybranych przypadków danych.
Metoda ta polega na generowaniu lokalnych wyjaśnień, które opisują, jakie cechy danych wejściowych wpływają na decyzje modelu.
Poprzez symulowanie zbliżonych instancji danych i analizę reakcji modelu na ich zmiany, LIME pozwala na zrozumienie, które cechy mają największy wpływ na wynik predykcji.

\begin{listing}
  \begin{minted}{c}
    int main()
    {
      int a=2*3;
    }
  \end{minted}
  \caption{Pseudo kod} \label{listing:lime}
\end{listing}

Kroki metody LIME:
\begin{itemize}
	\item Wybór instancji do wyjaśnienia - Na początku wybieramy konkretną instancję danych, której decyzję modelu chcemy wyjaśnić
	\item Generowanie perturbacji - Następnie generujemy zestaw perturbowanych wersji oryginalnej instancji.
	      Perturbacje te są tworzone przez wprowadzenie małych, losowych zmian do cech wejściowych.
	      W przypdaku danych obrazowych, perturbacje mogą polegać na zmianie wartości pikseli lub zamaskowaniu fragmentów obrazu.
	\item Ocena perturbowanych instancji - Każda perturbowana instancja jest następnie przekazywana do orginalnego model, aby uzyskać predykcję dla nich
	\item Ważenie perturbacji - Perturbacje są ważone na podstawie ich odległości od orginalnej instancji.
	      Perturbacje bliższe oryginalnej instancji mają większy wpływ na trenowanie lokalnego modelu.
	\item Tworzenie loalnego modelu - LIME buduje prosty, lokalny model interpretowalny (np. regresja liniowa) zw oparciu o perturbowane instancje i ich odpowiadające predykcje, z uwzględnieniem ich wag.
	      Ten model jest trenowany, aby najlepiej odwzorować zachowanie oryginalnego modelu w okolicy wybranej instancji.
	\item Generowanie wyjaśnień - Po zbudowaniu lokalnego modelu interpretowalnego, LIME identyfikuje najważniejsze cechy, które wpływają na predykcję oryginalnego modelu dla wybranej instancji.
	      Wyjaśnienia te są przedstawiane w postaci listy cech z przypisanymi wagami, które pokazują ich wpływ na decyzję modelu.
\end{itemize}

% SHAP
\subsubsection*{SHAP}
\textbf{SHAP} wykorzystuje teorię gier do przypisywania znaczenia poszczególnym cechom wejściowym w procesie decyzyjnym modelu.
Metoda ta opiera się na szacowaniu Shapley values, które określają, jaki wpływ ma każda cecha na przewidywaną wartość.
Dzięki tej technice możemy zidentyfikować, które cechy mają największy wpływ na wynik modelu i jakie są ich wzajemne zależności.

Teoria wartości Shapleya - wartości Shapleya pochodzą z teori gier kooperacyjnych i służą do podziału zysków (lub wpływów) między graczy (cechy w kontekście modelu) proporcjonalnie do ich wkładu w wynik.
Dla każdej cechy wartość Shapleya obliczana jest jako średni wkład tej cechy do wyniku modelu we wszystkich możliwych kombinacjach cech.

Kroki obliczania wartości Shapleya:
\begin{itemize}
	\item Perturbacja danych - Dla każdej cechy generowane są wszystkie możliwe podzbiory cech (kombinacjecech z wyjątkiem tej wybranej)
	\item Ocena wpływu cech - Model jest uruchamiany dla każdego podzbioru cech z i bez wybranej cechy, a różnice w wynikach są mierzone
	\item Średnia marginalna wartości - Wartości Shapleya dla danej cechy są obliczane jako średnia marginalnego wkładu tej cechy do wyniku modelu we weszystkich możliwych podzbiorach cech
\end{itemize}

Obliczanie dokładnych wartości Shapleya może być obliczeniowo kosztowne, dlatego SHAP wykorzystuje różne techniki przyspieszanie obliczeń, takie jak aproksymacje i metody samplingowe.

\vspace{1cm}
% podsumowanie
Metody Model-agnostic oferują elastyczne i uniwersalne podejście do wyjaśniania decyzji modeli, co czyni je atrakcyjnym narzędziem dla szerokiego spektrum zastosowań. Ich zdolność do pracy z różnymi rodzajami modeli oraz możliwość generowania lokalnych wyjaśnień pozwala na lepsze zrozumienie i interpretację działania modeli głębokiego uczenia.

\subsection*{Oparte na podejściu Model-specific}
% Wprowadzenie
Metody oparte na podejściu Model-specific to techniki XAI, które są związane bezpośrednio z architekturą i działaniem konkretnego modelu. Charakteryzują się one wysokim stopniem specjalizacji i dostosowania do konkretnych rodzajów modeli, co umożliwia uzyskanie bardziej szczegółowych i dokładnych wyjaśnień ich działania.

% GradCAM
GradCAM jest jedną z najbardziej popularnych metod Model-specific, która umożliwia wizualizację obszarów obrazu, które najbardziej wpływają na decyzję klasyfikacyjną modelu. Metoda ta wykorzystuje gradienty wsteczne, aby obliczyć istotność poszczególnych pikseli obrazu względem konkretnej klasy. Dzięki temu możemy zidentyfikować, które obszary obrazu były decydujące dla klasyfikacji i w jaki sposób model dokonywał swoich predykcji.

Metody Model-specific oferują wysoki poziom szczegółowości i precyzji w wyjaśnianiu działania konkretnych modeli, co czyni je atrakcyjnym narzędziem dla zrozumienia ich wewnętrznej logiki i mechanizmów decyzyjnych. Ich dostosowanie do konkretnych architektur modeli pozwala na uzyskanie bardziej precyzyjnych i zrozumiałych wyjaśnień, co jest kluczowe w przypadku zastosowań wymagających wysokiego stopnia pewności i zaufania do modeli.

\section*{Porównywanie metod XAI w klasyfikacji obrazów}
%Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing
%DHIS - Evaluation of Explainable Artificial Intelligence: SHAP, LIME, and CAM
%Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization
%CLEVER_XAI
Porównywanie metod wyjaśnialnej sztucznej inteligencji (XAI) w kontekście klasyfikacj obrazów jest kluczowe, aby zrozumieć, które techniki są najbardziej skuteczne i jakie mają zalety oraz ograniczenia.
Istnieją dwa główne podejścia do porównywania metod XAI: oparte na modelu (model-based) oraz oparte na prawdzie (truth-based).

\subsection*{Oparte na modelu}
Metody oparte na modelu oceniają wyjaśnienia XAI poprzez analiże ich wpływu na wyniki modelu predykcyjnego.
Metryki stosowane w tym podejściu:
\begin{itemize}
	\item Average drop in confidence and activation score - ta metryka mierzy średni spadek pewności modelu w przypadku usunięcia cech, które zostały zidentyfikowane jako ważne przez metodę XAI.
	      Jeżeli metoda XAI poprawnie identyfikuje istotne cechy, usunięci tych cech powinno prowadzić do znaczącego spadku pewności predykcji modelu.
	      Metryka ta jest używana do oceny, jak skutecznie metoda XAI identyfikuje krytyczne cechy wpływające na decyzję modelu.
	\item Percent increase in confidence and activation score - ta metryka mierzy średni procentowy wzrost pewności modelu, gdy tylko cechy zidentyfikowane jako ważne przez metodę XAI pozostają w obrazie, a wszystkie inne cechy są usunięte.
	      Jeśli metoda XAI prawidłowo identyfikuje istotne cechy, pozostawienie tylko tych cech powinno prowadzić do wzrostu pewności predykcji modelu.
	      Ta metryka pozwala ocenić, w jakim stopniu metoda XAI może poprawić zrozumienie krytycznych cech przez model.
\end{itemize}

\subsection*{Metody oparte na prawdzie}
Metody oparte na prawdzie oceniają wyjaśnienia XAI przez porównanie ich z zewnętrznymi, zdefinowanymi prawdziwymi danymi. Jedną z głównych metryk w tym podjściu jest:
\begin{itemize}
	\item Intersection over Union (IoU) - jest to miara pokrycia między wyjaśnieniami wygenerowanymi przez metodę XAI a zdefiniowanymi obszarami referencyjnymi w obrazie, które są uważane za istotne (np. ręcznie oznaczone przez ekspertów).
	      IoU jest obliczane jako stosunek powierzchni przecięcia do powierzchni sumy obszarów wyjaśnień i referencji. Wysokie IoU wskazuje na dobrą zgodność wyjaśnień XAI z rzeczywistością, co oznacza, że metoda XAI skutrcznie identyfikuje istotne cechy.
\end{itemize}
