\abstract{
	W pracy przeprowadzono analizę i porównanie metod wyjaśnialnej sztucznej inteligencji (XAI): LIME, SHAP oraz GradCAM, z uwzględnieniem przeglądu literatury.
	Celem było ocenienie spójności, dokładności oraz jakości wyjaśnień generowanych przez te metody.
	Badania obejmowały również łączenie wyjaśnień w celu próby popraw interpretowalności wyników.
	GradCAM wykazał się największą spójnością i dokładnością, zwłaszcza w dopasowywaniu wielkości obszarów wyjaśnień do rzeczywistych rozmiarów obiektów.
	LIME osiągnął najmniejszy odsetek obszarów wyjaśnień poza rzeczywistymi obiektami.
	Wyniki wskazują na potencjał łączenia różnych metod XAI dla poprawy interpretowalności modeli głębokich.

	W badaniach użyto zbioru danych ImageNet-9, który zawiera dokładne informacje o pozycji obiektów, co umożliwiło szczegółową analizę wyjaśnień metod XAI.
	Wykorzystano również architekturę modelu ResNet50, który jest powszechnie stosowaną architekturą klasyfikacji obrazów w głębokim uczeniu.
}{
	This paper analyzed and compared the explanatory artificial intelligence (XAI) methods LIME, SHAP and GradCAM, including a literature review.
	The goal was to assess the consistency, accuracy and quality of the explanations generated by these methods.
	The study also included combining explanations to try to improve the interpretability of the results.
	GradCAM showed the highest consistency and accuracy, especially in matching the size of the explanation areas to the actual sizes of the objects.
	LIME achieved the lowest percentage of explanation areas outside of the actual objects.
	The results indicate the potential of combining different XAI methods to improve the interpretability of deep models.

	The study used the ImageNet-9 dataset, which contains accurate information about the position of objects, which enabled detailed analysis of XAI method explanations.
	The ResNet50 model architecture was also used, which is a commonly used architecture for image classification in deep learning.
}
